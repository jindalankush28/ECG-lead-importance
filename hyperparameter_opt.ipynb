{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Conv1D, Dropout, Activation, Flatten\n",
    "from keras.metrics import AUC\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import Sequence\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Local imports\n",
    "from models import Attia_et_al_CNN\n",
    "\n",
    "from utils import split_train_val_test\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Environment variables\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle('data/arrythmia_dataset.pickle')\n",
    "conditions = ['1AVB', 'AF', 'AFIB', 'APB', 'AQW', 'IDC',\n",
    "              'LVH', 'LVQRSAL', 'RBBB', 'SR', 'ST',\n",
    "              'STDD', 'STE', 'STTC', 'SVT', 'TWC',\n",
    "              'TWO']\n",
    "\n",
    "output_size = len(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X, y = data_df['wf'].to_numpy(), data_df[conditions].to_numpy()\n",
    "y = y.astype(float)\n",
    "X = np.stack(X, axis=0)\n",
    "\n",
    "del data_df\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y mean: {y.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test(X, y, train_size=0.70, val_size=0.15)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, y, batch_size=8):\n",
    "    row_nums = np.arange(X.shape[0])\n",
    "    np.random.shuffle(row_nums)\n",
    "    for i in range(0, len(row_nums), batch_size):\n",
    "        current_idxs = row_nums[i:i+batch_size]\n",
    "\n",
    "        yield X[current_idxs], y[current_idxs,:]\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(None, 5000, 12), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, output_size), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(generator=lambda: generator(X_train,y_train, 8), output_signature=output_signature)\n",
    "val_ds = tf.data.Dataset.from_generator(generator=lambda: generator(X_val,y_val, 8), output_signature=output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(hyperparameters, train_ds, val_ds):\n",
    "    \n",
    "    #create the model\n",
    "    model = Attia_et_al_CNN(\n",
    "        filter_numbers = hyperparameters['filter_numbers'], \n",
    "        kernel_widths = hyperparameters['kernel_widths'], \n",
    "        pool_sizes = hyperparameters['pool_sizes'], \n",
    "        spatial_num_filters = hyperparameters['spatial_num_filters'], \n",
    "        dense_dropout_rate = hyperparameters['dense_dropout_rate'], \n",
    "        spatial_dropout_rate = hyperparameters['spatial_dropout_rate'], \n",
    "        dense_units = hyperparameters['dense_units'], \n",
    "        use_spatial_layer = hyperparameters['use_spatial_layer'], \n",
    "        verbose = hyperparameters['verbose'], \n",
    "        output_size = output_size,\n",
    "    ).build()\n",
    "    \n",
    "    #create optimizers and compile model\n",
    "    learning_rate =1e-3\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',  # Monitor validation loss\n",
    "        factor=0.5,  # Reduce learning rate by half when triggered\n",
    "        patience=3,  # Number of epochs with no improvement to trigger the callback\n",
    "        verbose=1,  # Print messages\n",
    "        min_lr=1e-8  # Minimum learning rate\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6, mode='min', restore_best_weights=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=['accuracy', AUC(name='auc')])\n",
    "    # Training parameters\n",
    "    EPOCHS = 50  # You can adjust based on your needs\n",
    "    \n",
    "    #fit the model\n",
    "    history = model.fit(train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[reduce_lr, early_stopping],\n",
    "        verbose=1)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #create dictionary of roc auc curves\n",
    "    roc_auc_scores_dict = {}\n",
    "    \n",
    "    #evaluate the model\n",
    "    for i, condition in enumerate(conditions):\n",
    "        auc = roc_auc_score(y_test[:,i], y_pred[:,i])\n",
    "        roc_auc_scores_dict[condition] = auc\n",
    "        \n",
    "    #return the model hyperparameters and the roc_auc scores\n",
    "    return hyperparameters, roc_auc_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hype1 = {\n",
    "    'filter_numbers' : [16, 16, 32, 32, 64, 64],\n",
    "    'kernel_widths' : [5, 5, 5, 3, 3, 3],\n",
    "    'pool_sizes' : [2, 2, 4, 2, 2, 4],\n",
    "    'spatial_num_filters' : 64,\n",
    "    'dense_dropout_rate' : 0.2,\n",
    "    'spatial_dropout_rate' : 0.2,\n",
    "    'dense_units' : [64, 32],\n",
    "    'use_spatial_layer' : False,\n",
    "    'verbose' : 1,\n",
    "    'output_size' : output_size,\n",
    "}\n",
    "\n",
    "hype2 = {\n",
    "    'filter_numbers' : [16, 16, 32],\n",
    "    'kernel_widths' : [5, 5, 5],\n",
    "    'pool_sizes' : [2, 2, 4],\n",
    "    'spatial_num_filters' : 64,\n",
    "    'dense_dropout_rate' : 0.2,\n",
    "    'spatial_dropout_rate' : 0.2,\n",
    "    'dense_units' : [64, 32],\n",
    "    'use_spatial_layer' : False,\n",
    "    'verbose' : 1,\n",
    "    'output_size' : output_size,\n",
    "}\n",
    "\n",
    "hyper_param_ls = [hype2, hype1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_to_auc_roc = {}\n",
    "\n",
    "for h in hyper_param_ls:\n",
    "    hyperparameters, roc_auc_scores_dict = train_single_model(h, train_ds, val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
